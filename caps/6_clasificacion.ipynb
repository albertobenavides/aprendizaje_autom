{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Los problemas de clasificación buscan predecir las categorías a las que pertenecen las variables dependientes. Por ejemplo, saber si un cliente será moroso, si un paciente tiene cierta enfermedad, etc. son problemas de clasificación. \n",
    "\n",
    "Los resultados de los modelos de clasificación pueden ser las clases asignadas o probabilidades de pertenencia a cierta clase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores lineales\n",
    "\n",
    "Como ejemplo, empecemos con un método de clasificación lineal basado en funciones discriminantes. Este modelo busca asignar $K = \\{1, 2, ..., k\\}$ clases mediante la función de ajuste \n",
    "\n",
    "$$\\hat{f}_k(x) = \\beta_{k,0} + \\beta_k x.$$ \n",
    "\n",
    "Para cada par de clases $i, j$, existen cotas tales que $\\hat{f}_i(x) = \\hat{f}_j(x)$. De esta manera, el espacio de solución se divide en hiperplanos que corresponden con regiones donde clasificar las variables.\n",
    "\n",
    "Otro tipo de modelos lineales de clasificación que entran dentro de las funciones discriminantes, son las que parten de distribuciones de probabilidad _a posteriori_ del tipo \n",
    "\n",
    "$$P(G = k | X = x),$$ \n",
    "\n",
    "donde $G$ es el predictor que se espera sea asignado a la clase $k$ a partir de que la información observada $X$ para las variables dependientse $x$.\n",
    "\n",
    "Cabe señalar que, pese a que las cotas obtenidas de las funciones de ajuste $\\hat{f}_k(x)$ resulten lineales, es posible generalizarlas mediante el uso de transformaciones de las variables $p-\\text{dimensionales}$ $X_i, i \\in {1, \\ldots, p}$ para expandir el número de variables (con base en la transformación aplicada sobre las varaibles). Estas transformaciones generalmente asignan vectores de valores $C$ y pesos $W$ a las características $q$ (o datos de entrada), que se traducen en multiplicaciones matriciales $C_q \\times W_q$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador de Bayes ingenuo\n",
    "\n",
    "El clasificador de Bayes ingenuo (o *Naive Bayes*) se puede entender desde la premisa de un clasificador bayesiando exacto, en el que \n",
    "1. se toman vectores de variables independiente $X$ que tengan la misma salida $Y$,\n",
    "2. se determina la clase predominante del conjunto, y\n",
    "3. se utiliza esta información para asignar el valor de la clase más frecuente a nuevos elementos.\n",
    "\n",
    "Esta solución podría estar sesgada a las características de los conjuntos. Para paliar dicho sesgo, *Naive Bayes* sigue estas modificaciones:\n",
    "- Se usan todo el conjunto de datos.\n",
    "- Para cada respuesta $Y = {1, \\ldots, k}$, se estima la probabilidad condicional del predictor $P(X_j | Y = i)$\n",
    "- Se realiza la probabilidad condicional \n",
    "\n",
    "$$P(Y = i | X_1, \\ldots, X_j) = \\frac{P(Y = i) \\cdot P(X_1 | Y = i) \\ldots P(X_j | Y = i)}{\\sum_i P(Y = i) P(X_1, \\ldots, X_j)}.$$\n",
    "\n",
    "- Se asume que las $X$ son independientes entre sí (al obtener la probabilidad de interacción con $Y$ de cada una de ellas)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis lineal de discriminantes\n",
    "\n",
    "Este análisis (LDA en inglés, por *linear discriminant analysis*), parte de la covarianza entre $n$ datos $x_i, z_i; i \\in {1, \\ldots, n}$ dada por $$s_{x, z} = \\frac{\\sum_i^n (x_i - \\bar{x}) (z_i - \\bar{z})}{n - 1},$$ para calcular la matriz de covarianza \n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}\n",
    "s_x^2 & s_{x, z} \\\\ \n",
    "s_{z, x} & s_z^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "que incluye las varianzas individuales $s_x^2, s_z^2$ y sus covarianzas $s_{, z}, s_{z, x}$.\n",
    "\n",
    "LDA se enfoca en maximizar la separación entre las medias de las clases $(\\bar{x} - \\bar{z})^2$ y minimizar la similitud entre miembros de una misma clase, o sea $s_x^2 + s_z^2$, de modo que la función objetivo sería \n",
    "\n",
    "$$\\max \\frac{(\\bar{x} - \\bar{z})^2}{s_x^2 + s_z^2}.$$\n",
    "\n",
    "Para el caso generalizado de $p$ dimensiones con $X_i, i \\in {1, \\ldots, p}$, se toma como punto de referencia el centro $C$ de los datos dado por la media ponderada de todos los datos. Para cada $X_i$ se mide la distancia $d_i = (\\bar{X_i} - C)^2$, para obtener una función objetivo \n",
    "\n",
    "$$\\max \\sum_i \\frac{(\\bar{X_i} - C)^2}{s_{X_i}^2}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de decisión\n",
    "\n",
    "En general, los árboles de decisión clasifican datos $X$ a partir de su separación en $n$ regiones $R$ y obtienen una clasificación $Y$ a partir de las cotas que limitan las regiones. Una vez obtenidas dichas regiones, la función de predicción \n",
    "\n",
    "$$f(X) = \\sum_m^n c_m ~ (X_i, X_j) \\in R_m.$$\n",
    "\n",
    "Específicamente, en el modelo de clasificación de árboles de decisión, se utiliza una proporción de observaciones \n",
    "\n",
    "$$p_{mk} = \\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = k),$$\n",
    "\n",
    "donde $N_m$ es la cantidad de observaciones y $k \\in K$ clases. Después, se clasifican las observaciones $m$ en la clase $k$ mediante la maximización de esa proporción, es decir $$k(m) = \\arg \\max x_k p_{mk}.$$ Finalmente, se tienen medidas acerca de la impureza de los nodos, como por ejemplo el índice Gini que mide la probabilidad de que una observación $m$ pertenezca a una clase $k$ respecto a que no pertenezca ($k'$) a dicha clase, dada por la ecuación \n",
    "\n",
    "$$\\sum_{k \\neq k'} p_{mk} p_{mk'} = \\sum_{k = 1}^K p_{mk}(1 - p_{mk}).$$ \n",
    "\n",
    "Para darle un poco de claridad, para una clase, esto puede escribirse como \n",
    "\n",
    "$$1 - (p_{mk})^2 - (p_{mk'})^2.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bosques aleatorios\n",
    "\n",
    "Ver pizarra."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea (10 puntos)\n",
    "\n",
    "- Generar la parte de metodología del artículo, explicando los métodos usados con base en la literatura que los sustenta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73cfb11ebb5098915952b858e6200bb38d79de5587d8fc0575441f97863c5189"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
