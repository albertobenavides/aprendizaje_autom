#!/usr/bin/env python
# coding: utf-8

# # Selecci√≥n de caracter√≠sticas
# 
# T√©cnica utilizada para eliminar caracter√≠sticas que sean estad√≠sticamente redundantes o aporten poca informaci√≥n a los modelos. Como consecuencia, suelen disminuir tiempos de entrenamiento de modelos con muchas caracter√≠sticas e incluso mejorar sus resultados.

# ## M√©todos de filtro
# 
# Usan estad√≠sticos para determinar umbrales sobre los que elegir caracter√≠sticas. Suelen ser m√°s r√°pidos que otros m√©todos, mas no suelen incluir interacci√≥n entre variables.

# ### ANOVA de valor $F$
# 
# Determina linealidad entre variables de entrada y salida. Un valor $F$ alto, indica alta relaci√≥n lineal; valores menores, lo contrario.

# Primero cargamos los paquetes.

# In[1]:


import pandas as pd

import matplotlib.pyplot as plt

# Ignorar advertencias
import warnings
warnings.filterwarnings('ignore')


# Luego los datos.

# In[2]:


df_pollutants_coords = pd.read_csv('../data/results/df_pollutants_coords.csv')
df_pollutants_coords.timestamp = pd.to_datetime(df_pollutants_coords.timestamp)
df_pollutants_coords.dropna(inplace = True)
df_pollutants_coords


# Para esta sesi√≥n, se busca explicar el contaminante $\text{PM}_{10}$ como $Y$ a partir de las variables atmosf√©ricas $X$, independientes del tiempo y geolocalizaci√≥n.

# In[3]:


y = df_pollutants_coords[['PM10']]
y


# In[4]:


x = df_pollutants_coords[['BP', 'RF', 'RH', 'SR', 'T', 'WV', 'WD']]
x


# En Python se puede usar `f_regression` para calcular el valor $F$ en regresiones, mientras que `f_classif` se usa para clasificaciones.

# In[5]:


from sklearn.feature_selection import f_regression


# In[6]:


f_value = f_regression(x, y)
# Regresa arreglo de estad√≠stico F y valor p
f_value


# Determinar si las variables aportan linealmente informaci√≥n relevante al modelo.

# In[7]:


pass_test = []
not_pass_test = []
alpha = 0.05
for i in range(len(f_value[1])):
    print(x.columns[i], f_value[1][i])
    if f_value[1][i] < alpha:
        pass_test.append(x.columns[i])
    else:
        not_pass_test.append(x.columns[i])


# In[8]:


df_results = pd.DataFrame(f_value[0], index=x.columns)
df_results.columns = ['f_value']
df_results.sort_values('f_value', inplace = True, ascending = False)
df_results


# Los resultados. Como recordatorio: Barra m√°s alta, m√°s linealidad con $\text{PM}_{10}$.

# In[9]:


plt.figure()
plt.bar(df_results.drop(not_pass_test).index, df_results.drop(not_pass_test).f_value)
plt.show()


# La humedad relativa, direcci√≥n del viento y radiaci√≥n solar encabezan las variables m√°s linealmente relacionadas con $\text{PM}_{10}$.

# ### Valor $R$ de correlaci√≥n
# 
# La correlaci√≥n ya se estudi√≥ en el cap√≠tulo pasado. Tambi√©n se puede utilizar como selecci√≥n de caracter√≠sticas.

# In[10]:


from sklearn.feature_selection import r_regression


# In[11]:


r_value = r_regression(x, y)
r_value


# In[12]:


df_results['r_value'] = r_value
colors = []
for v in df_results['r_value']:
    if v > 0:
        colors.append('b')
    else:
        colors.append('r')


# In[13]:


df_results['r_value_abs'] = df_results['r_value'].abs()
df_results.sort_values('r_value_abs', inplace = True, ascending = False)
plt.figure()
plt.bar(df_results.index, df_results.r_value_abs, color = colors)
plt.show()


# ### Umbral de varianza
# 
# Otro modelo de filtro para selecci√≥n de caracter√≠sticas es el umbral de varianza, que consiste en descartar caracter√≠sticas con baja varianza, en el supuesto de que no aportan tanta informaci√≥n al modelo. Requiere que las caracter√≠sticas est√©n normalizadas.

# In[14]:


# Normalizaci√≥n de variables
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled = scaler.fit_transform(x)
x_scaled = pd.DataFrame(scaled, columns = x.columns)
x_scaled


# In[15]:


from sklearn.feature_selection import VarianceThreshold


# In[16]:


selector = VarianceThreshold()
selector.fit_transform(x_scaled)
selector.variances_


# In[17]:


# Se agregan las varianzas a los resultados
df_results['variance'] = selector.variances_
df_results.sort_values('variance', ascending = False, inplace = True)
df_results


# In[18]:


plt.figure()
plt.bar(df_results.index, df_results.variance)
plt.show()


# Generalmente, se suelen eliminar caracter√≠sticas con varianza menor a $0.2$. Aqu√≠ todas ser√≠an eliminadas üòÖ

# ### Informaci√≥n mutua
# 
# Este modelo mide la dependencia entre variables. Un valor de $0$ indicar√≠a que las variables son independientes. Este modelo captura relaciones no lineales entre las variables üëÄ
# 
# Aqu√≠ tambi√©n existe la variante `classif` para modelos que impliquen clasificaci√≥n.

# In[19]:


from sklearn.feature_selection import mutual_info_regression


# In[20]:


mi = mutual_info_regression(x, y, random_state=0)
mi


# In[21]:


# Agregarlo a los resultados
df_results['mi'] = mi
df_results.sort_values('mi', ascending = False, inplace = True)

plt.figure()
plt.bar(df_results.index, df_results.mi)
plt.show()


# Una vez que se tienen algunos resultados, es recomendable utilizar una m√©trica para tomar una decisi√≥n. Hay muchas maneras de hacer esto, por ejemplo mediante la media de los valores normalizados. De esta manera, tenemos s√≥lo una variable de decisi√≥n.

# In[22]:


scaled = scaler.fit_transform(df_results)
df_results_scaled = pd.DataFrame(scaled, columns = df_results.columns)
df_results_scaled.set_index(df_results.index, inplace = True)
df_results_scaled['norm_mean'] = df_results_scaled.mean(axis = 1)
df_results_scaled.sort_values('norm_mean', ascending = False, inplace = True)

plt.figure()
plt.bar(df_results_scaled.index, df_results_scaled.mean(axis = 1))
plt.show()


# ## M√©todos de envoltura (?) o *wrapper*

# M√©todos que exploran subconjuntos de combinaciones de caracter√≠sticas que mejoren alg√∫n desempe√±o de modelos de AA, con la ventaja de que, al usar un modelo, se estudian las relaciones de las carracter√≠sticas *en* el modelo, a diferencia de los m√©todos de filtro, donde la relaci√≥n de caracter√≠sticas depend√≠a de estad√≠sticos. Estos m√©todos tienen la desventaja de que, a mayor complejidad del modelo y n√∫mero de caracter√≠sticas, mayor consumo de recursos y tiempos de ejecuci√≥n ‚åõ

# ### Selecci√≥n de caracter√≠sticas exhaustiva
# 
# La Selecci√≥n de caracter√≠sticas exhaustiva o EFS (*Exhaustive Feature Selection*) por sus siglas en ingl√©s eval√∫a todas las combinaciones de caracter√≠sticas y devuelve los valores que optimizan el modelo. Como ejemplo, se usa una regresi√≥n lineal.

# In[23]:


from sklearn.linear_model import LinearRegression


# In[24]:


from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS


# In[25]:


lr = LinearRegression()

efs = EFS(estimator = lr,        # Use logistic regression as the classifier/estimator
          min_features = 1,      # The minimum number of features to consider is 1
          max_features = 7,      # The maximum number of features to consider is 4
          scoring = 'neg_mean_absolute_error',  # The metric to use to evaluate the classifier is accuracy 
          cv = 5)


# In[26]:


efs = efs.fit(x, y)


# In[27]:


print('Best accuracy score: %.2f' % efs.best_score_)
# print('Best subset (indices):', efs.best_idx_)
print('Best subset (corresponding names):', efs.best_feature_names_)


# In[28]:


metric_dict = efs.get_metric_dict()
df_efs = pd.DataFrame(metric_dict).T
df_efs.sort_values('avg_score', ascending=False,  inplace = True)
df_efs_best_10 = df_efs.iloc[:10]
df_efs_best_10


# In[29]:


# https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/#example-2-visualizing-the-feature-selection-results

fig = plt.figure(figsize=(20, 4))

plt.plot(
    df_efs_best_10.feature_names.astype(str), 
    df_efs_best_10.avg_score, 
    color='blue', marker='o'
)
plt.ylabel('MAE')
plt.xlabel('Caracter√≠sticas')

plt.xticks(rotation = 90)

plt.show()


# ### Sequential Forward Selection (SFS)
# 
# Este modelo agrega en cada iteraci√≥n una variable e identifica las variables que mejoran la m√©trica del modelo.

# In[30]:


from mlxtend.feature_selection import SequentialFeatureSelector as SFS


# In[31]:


sfs = SFS(estimator = lr,        # Use logistic regression as the classifier/estimator
          k_features = (1, 7),  # Consider any feature combination between 1 and 8
          forward = True,       # Set forward to True when we want to perform SFS
          scoring = 'neg_mean_absolute_error',  # The metric to use to evaluate the classifier is accuracy 
          cv=5)


# In[32]:


sfs = sfs.fit(x, y)


# In[33]:


print('Best accuracy score: %.2f' % sfs.k_score_)   # k_score_ shows the best score 
print('Best subset (corresponding names):', sfs.k_feature_names_) # k_feature_names_ shows the feature names 
                                                                  # that yield the best score


# In[34]:


subsets_ = sfs.subsets_
df_sfs = pd.DataFrame(subsets_).T
df_sfs.sort_values('avg_score', ascending=False,  inplace = True)
df_sfs


# In[35]:


fig = plt.figure(figsize=(20, 4))

plt.plot(
    df_sfs.feature_names.astype(str), 
    df_sfs.avg_score, 
    color='blue', marker='o'
)
plt.ylabel('MAE')
plt.xlabel('Caracter√≠sticas')

plt.xticks(rotation = 90)

plt.show()


# ### Sequential Backward Selection (SBS)
# 
# Lo mismo, pero al rev√©s üòõ

# In[36]:


sbs = SFS(estimator = lr,
          k_features=(1, 7),
          forward=True,
          scoring='neg_mean_absolute_error',
          cv=5)

sbs = sbs.fit(x, y)
subsets_ = sbs.subsets_
df_sbs = pd.DataFrame(subsets_).T
df_sbs.sort_values('avg_score', ascending=False,  inplace = True)

fig = plt.figure(figsize=(20, 4))
plt.plot(
    df_sbs.feature_names.astype(str), 
    df_sbs.avg_score, 
    color='blue', marker='o'
)
plt.ylabel('MAE')
plt.xlabel('Caracter√≠sticas')

plt.xticks(rotation = 90)

plt.show()


# ## PCA
# 
# El [an√°lisis de componentes principales](https://www.cienciadedatos.net/documentos/py19-pca-python.html) es una especie de t√©cnica de reducci√≥n de caracter√≠sticas que podr√≠a utilizarse como selecci√≥n de caracter√≠sticas. Consiste en reducir la dimensionalidad de caracter√≠sticas mediante hiperpar√°metros que incluyan las caracter√≠sticas que m√°s varianza tengan para explicar un modelo.

# In[37]:


from sklearn.decomposition import PCA
import numpy as np


# In[38]:


pca = PCA(n_components = 3)
pca_model = pca.fit(x_scaled)


# In[39]:


plt.figure()
plt.bar(np.arange(pca_model.n_components_) + 1, pca_model.explained_variance_ratio_)
prop_varianza_acum = pca_model.explained_variance_ratio_.cumsum()
plt.plot(range(1, 4),prop_varianza_acum, marker = 'o', c='orange', label='Var. acumulada')
plt.xticks(np.arange(pca_model.n_components_) + 1)
plt.ylim(0, 1.1)
plt.xlabel('Componente principal', fontsize=12)
plt.ylabel('Varianza explicada', fontsize=16)
plt.legend()
plt.show()


# Para conocer los coeficientes que utiliza PCA para sus componentes, se puede hacer lo siguiente.

# In[40]:


# Coeficientes del PCA
pca_coef = pd.DataFrame(
  data    = pca_model.components_,
  columns = x_scaled.columns,
  index = ['pca1', 'pca2', 'pca3']
).T.sort_values('pca1', ascending=False)
pca_coef


# In[41]:


formula = ''
for i, r in pca_coef.iterrows():
    formula = formula +  str(round(r.pca1, 4)) + ' \text{' + i + '} + '
formula


# $$\text{PCA}_1 = 0.506 \cdot \text{SR} + 0.4066 \cdot \text{T} + 0.2737 \cdot \text{WV} + -0.0013 \cdot \text{RF} + -0.3749 \cdot \text{BP} + -0.4157 \cdot \text{RH} + -0.4362 \cdot \text{WD}.$$

# - [ ] M√©todos embebidos

# ## Tarea en clase (2 puntos)
# 
# - Aplica alg√∫n m√©todo de filtro a tus datos mediante el uso de [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)

# ## Tarea (8 puntos)
# 
# - Aplica los modelos de selecci√≥n de caracter√≠sticas cuidando los supuestos de cada modelo
# - Busca una o varias m√©tricas para seleccionar caracter√≠sticas en literatura relacionada con tu problema (cita tus fuentes)
# - Con base en tu investigaci√≥n, determina las caracter√≠sticas m√°s relevantes de tu conjunto de datos
# - Discute por qu√© crees que las caracter√≠sticas seleccionadas son las m√°s relevantes y por qu√© el resto quedaron excluidas en la selecci√≥n

# ## Referencias
# - https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
# - https://neptune.ai/blog/feature-selection-methods
# - https://www.simplilearn.com/tutorials/machine-learning-tutorial/feature-selection-in-machine-learning
# - https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/
# - https://scikit-learn.org/stable/modules/feature_selection.html
# - https://www.kaggle.com/code/ar2017/basics-of-feature-selection-with-python
# - https://www.blog.trainindata.com/feature-selection-machine-learning-with-python/
# - https://towardsdatascience.com/beyond-linear-regression-467a7fc3bafb
# - https://github.com/AutoViML/featurewiz ‚≠ê
