#!/usr/bin/env python
# coding: utf-8

# # M茅tricas de desempe帽o
# 
# 

# ## Clasificaci贸n

# ### Matriz de confusi贸n

# Una matriz de confusi贸n mide la cantidad de clasificaciones correctas e incorrectas en un modelo. Estas predicciones aparecen en forma de matriz, tal que

# In[1]:


import numpy as np

from sklearn import metrics

import matplotlib.pyplot as plt


# In[2]:


actual = np.random.binomial(1, 0.9, size = 10000)
predicted = np.random.binomial(1, 0.9, size = 10000)
confusion_matrix = metrics.confusion_matrix(actual, predicted)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Falso', 'Verdadero'])


# In[3]:


plt.figure()
cm_display.plot()
plt.ylabel('Valor real $Y$')
plt.xlabel('Valor predecido $\hat{Y}$')
plt.show()


# En esta matriz, se tienen cuatro posibilidades
# - un *verdadero positivo* (VP) son los resultados correctamente clasificado como positivo;
# - un *verdadero negativo* (VN) se trata de los resultados correctamente clasificado como negativo;
# - un *falso positivo* (FP) son los errores de clasificaci贸n de resultado positivo, conocido como Error tipo I; y
# - un *falso negativo* (FN) representa los errores de clasificaci贸n de resultado negativo, o Error tipo II.

# A partir de estos valores, se pueden calcular
# - la **exactitud** (_accuracy_) como la proporci贸n de clasificaciones positivas que fueron correctas, es decir $(VP + VN) / (VP + VN + FP + FN)$;
# - la **precisi贸n** o **valor predictivo positivo** (_precision_ o _positive predictive value_) en tanto la proporci贸n de clasificaciones verdaderas positivas entre las clasificaciones verdaderas, o sea $VP / (VP + FP)$;
# - la **sensibilidad** (_recall_ o _sensitivity_) corresponde a la proporci贸n de verdaderos positivos dados todos los positivos, a saber $VP / (VP + FN)$; y
# - la **especificidad** (_specificity_) es la proporci贸n de verdaderos negativos, tal que $VN / (VN + FP)$.
# 
# Tambi茅n es posible calcular la **tasa de falsos negativos** (o tasa de error) como $FN / (FN + VP)$, el  $VP / (VP + FN)$.

# ### Media-$G$
# 
# Se llama $G$-mean en ingl茅s y se usa cuando hay una gran desproporci贸n entre los elementos clasificados positiva y negativamente ($80\%/20\%$, por ejemplo). Se calcula mediante $\sqrt{\text{sensibilidad} \cdot \text{precisi贸n}}$.

# ### Valor-$F$

# Se trata de obtener la mejor precisi贸n y sensibilidad, mediante la f贸rmula
# 
# $$
# F_1 = 2 \cdot \frac{\text{sensibilidad} \cdot \text{precisi贸n}}{\text{sensibilidad} + \text{precisi贸n}}.
# $$
# 
# El valor-$F$ pertenece al intervalo de soluci贸n $[0, 1]$. Cuando el valor_$F = 1$, se tiene la mejor sensibilidad y precisi贸n, lo que quiere decir que no se equivoca en clasificar positivos, pr谩cticamente, o tambi茅n se puede interpretar como que un modelo clasifica un valor-$F$ de las veces los valores positivos.

# ### $F_{\beta}$

# Para dar una proporci贸n distinta entre los valores de sensibilidad y precisi贸n en $F_1$, se puede asignar un coeficiente $\beta$ a la precisi贸n, de modo que 
# 
# $$
# F_\beta = (1 + \beta) \cdot \frac{\text{sensibilidad} \cdot \text{precisi贸n}}{\text{sensibilidad} + (\beta^2 \cdot \text{precisi贸n})}.
# $$

# ### AUC y ROC

# El 谩rea bajo la curva (_Area Under the Curve_) y la caracter铆stica operativa del receptor  (_Receiver Operating Characteristic_) representan gr谩ficamente la capacidad de un modelo de clasificar los verdaderos positivos, dada la gr谩fica

# In[4]:


# https://www.themachinelearners.com/curva-roc-vs-prec-recall/
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=10000, n_classes=2)
trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.7)

model = LogisticRegression()
model.fit(trainX, trainy)

# Predecimos las probabilidades
lr_probs = model.predict_proba(testX)

#Nos quedamos con las probabilidades de la clase positiva (la probabilidad de 1)
lr_probs = lr_probs[:, 1]

ns_probs = [0 for _ in range(len(testy))]

ns_fpr, ns_tpr, _ = metrics.roc_curve(testy, ns_probs)
lr_fpr, lr_tpr, _ = metrics.roc_curve(testy, lr_probs)

plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Sin entrenar')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Regresi贸n Log铆stica')

# Etiquetas de los ejes
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.legend()
plt.show()


# Los valores arriba y a la izquierda de la l铆nea punteada son los valores con mejor sensibilidad (tasa de verdaderos positivos) y menor tasa de falsos positivos. Los valores sobre la l铆nea punteada representan la misma cantidad de verdaderos positivos frente a falsos positivos. Si hubiera valores por debajo de esa l铆nea, ser铆an predicciones peores que el azar.

# ## Regresi贸n

# ### MSE
# 
# Error cuadr谩tico medio (_Mean Squared Error_) se calcula como
# 
# $$
# \text{MSE} = \frac{\sum_i (Y - \hat{Y})^2}{N}.
# $$

# ### RMSE
# 
# Error de ra铆z cuadrada media (_Root Mean Squared Error_), dado por
# 
# $$
# \text{RMSE} = \sqrt{\frac{\sum_{i} (Y - \hat{Y})^2}{N}}.
# $$

# ### $R^2$
# 
# Mide qu茅 tan bueno es un modelo con base en la predicci贸n a partir de la media.
# 
# $$
# R^2 = 1 - \frac{\text{MSE}}{\sum_i (\bar{Y} - \hat{Y})^2}.
# $$

# ### MAE
# 
# Error absoluto medio (_Mean Absolute Error_) se calcula como
# 
# $$
# \text{MSE} = \frac{\sum_i |Y - \hat{Y}|}{N}.
# $$

# ### MAPE
# 
# Error absoluto medio (_Mean Absolute Error_) se calcula como
# 
# $$
# \text{MSE} = \frac{100}{N} \cdot\frac{\sum_i |Y - \hat{Y}|}{Y}.
# $$

# ## Tarea (10 puntos)
# 
# - Revisa en la literatura las m茅tricas de desempe帽o que se utilizan en tu problema.
# - Elige al menos una m茅trica para aplicar a tus modelos.
# - Reporta estos hallazgos en el marco te贸rico de tu art铆culo.
